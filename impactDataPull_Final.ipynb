{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6967f8b",
   "metadata": {},
   "source": [
    "# NAS IMPACT DATABASE SCRAPER\n",
    "This code scrapes impact data from the Nonindingenous Aquatic Species (https://nas.er.usgs.gov/) database. Rather than connecting from backend, this pulls from the frontend of the data entry portal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7cf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd                                           # handle dataframes\n",
    "import openpyxl                                               # import and export Excel files\n",
    "from bs4 import BeautifulSoup                                 # read HTML\n",
    "import time                                                   # set timers\n",
    "import re                                                     # clean text data\n",
    "from datetime import date                                     # get date to organize output files\n",
    "from rpy2.robjects.packages import importr                    # import packages from R - needed for taxize package\n",
    "from selenium import webdriver                                # automate web browser interaction\n",
    "from selenium.webdriver.common.keys import Keys               # automate keyboard actions\n",
    "from selenium.webdriver.common.by import By                   # find elements by html id on webpage\n",
    "from selenium.webdriver.support.ui import Select              # automate dropdown selection\n",
    "from selenium.common.exceptions import WebDriverException     # deal with exptions in webdriver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80565cd6",
   "metadata": {},
   "source": [
    "### Open Chrome Web Driver\n",
    "Opens to NAS sign in page. Manually enter personal login information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22154fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start web driver - use login information\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://nas.er.usgs.gov/Signin.aspx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7f483",
   "metadata": {},
   "source": [
    "### Pull NAS Impact Data\n",
    "This scrapes NAS impact data in two parts. It first scapes the tables on the NAS impacts list page. Unfortunantly, data from tables is incomplete. The second step scrapes the full impact description statement, notes, and geographic location from each impact ID page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f51e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact Info Time: 64\n"
     ]
    }
   ],
   "source": [
    "# set timer\n",
    "time_start = time.time()\n",
    "\n",
    "# function to extract table from HTML\n",
    "def find_table():\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    data_table = soup.find('table', id='ContentPlaceHolder1_gvImpacts')\n",
    "    return data_table\n",
    "\n",
    "# function to extract and append table data\n",
    "def extract_table_data(table):\n",
    "    for row in table.find_all('tr')[3:-2]:\n",
    "        row_data = []\n",
    "        for index, cell in enumerate(row.find_all('td')[1:-1]):\n",
    "            if index == 8:\n",
    "                checkmark_image = cell.find('img')\n",
    "                if checkmark_image:\n",
    "                    row_data.append('yes')\n",
    "                else:\n",
    "                    row_data.append('')\n",
    "            else:\n",
    "                row_data.append(cell.get_text().strip())\n",
    "        data.append(row_data)\n",
    "    return data\n",
    "\n",
    "# create empty list to store table data\n",
    "data = []\n",
    "\n",
    "# create list - this will allow code to click through impact database table pages\n",
    "# n is based on number of pages and will likely need to be manually updated - consider creating code in future\n",
    "n = 75\n",
    "table_clicks = list(range(2, 12)) + (list(range(4, 14)) * n)\n",
    "\n",
    "# extract table headers\n",
    "driver.get('https://nas.er.usgs.gov/DataEntry/Impact/ImpactsList.aspx')\n",
    "table = find_table()\n",
    "header = [th.text.strip() for th in table.find_all('th')[1:-1]]\n",
    "\n",
    "# extract data from first table\n",
    "extract_table_data(table)\n",
    "\n",
    "# extract data from remaining tables\n",
    "try:\n",
    "    for i in table_clicks:\n",
    "        element = f'/html/body/form/div[4]/div/table/tbody/tr[1]/td/table/tbody/tr/td[{i}]/a'\n",
    "        link = driver.find_element(By.XPATH, element)\n",
    "        link.click()\n",
    "        table = find_table()\n",
    "        extract_table_data(table)\n",
    "except:\n",
    "    print(f\"An error occurred:{e}\")\n",
    "    pass\n",
    "\n",
    "# create data table    \n",
    "impact_data = pd.DataFrame(data, columns = header)\n",
    "\n",
    "# remove any potiential duplicates based on Impact ID\n",
    "impact_data.drop_duplicates(subset='Impact ID', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# !! not all data appears completely in data tables - need to going into edit option\n",
    "      \n",
    "# create new column for Geographic Location\n",
    "impact_data['Geographic Location'] = None\n",
    "\n",
    "# go through each individual impact to get full Impact Description, Notes, and Geographic Location\n",
    "for index, row in impact_data.iterrows():\n",
    "    try:\n",
    "        url = 'https://nas.er.usgs.gov/DataEntry/Impact/Impacts.aspx?ImpactID=' + str(row['Impact ID'])\n",
    "        driver.get(url)\n",
    "        impact_description = driver.find_element(By.XPATH, '/html/body/form/div[4]/div/div/table[2]/tbody/tr[6]/td[2]/textarea' )\n",
    "        impact_data.at[index, 'Impact Description'] = impact_description.get_attribute('value')\n",
    "        notes = driver.find_element(By.XPATH, '/html/body/form/div[4]/div/div/table[2]/tbody/tr[7]/td[2]/textarea')\n",
    "        impact_data.at[index, 'Notes'] = notes.get_attribute('value')\n",
    "        geographic_loc = driver.find_element(By.XPATH, '/html/body/form/div[4]/div/div/table[2]/tbody/tr[9]/td[2]/input')\n",
    "        impact_data.at[index, 'Geographic Location'] = geographic_loc.get_attribute('value')\n",
    "    except:\n",
    "        driver.refresh()\n",
    "        index -= 1\n",
    "        continue\n",
    "\n",
    "print(f'Impact Info Time: {round((time.time()- time_start)/60)}')\n",
    "\n",
    "# create an output file with all impacts - this ensures database does not need to be scraped again if \n",
    "# work needs to be done later on    \n",
    "impact_data.to_excel('output_' + str(date.today()) + '.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8381809",
   "metadata": {},
   "source": [
    "### Edit NAS Impact Data\n",
    "Scientific and common names are added to NAS impact data. Furthermore, the TSN text strings are split apart and separated in individual rows. This improves GLANSIS teams ability to find which invasives are affecting individual impacted species. Text and columns are cleaned and rearranged for easier reading and viewing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b526c",
   "metadata": {},
   "source": [
    "If you get this error: \n",
    "\n",
    "\"Error:\n",
    "! The `x` argument of `as_tibble()` can't be missing as of tibble 3.0.0.\n",
    "Run `rlang::last_error()` to see where the error occurred.\"\n",
    "\n",
    "The ITIS website is likely down. Try again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5418fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code uploads archived version of impact database\n",
    "# impact_data = pd.read_excel(r'output_.xlsx', dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f3cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a species list with all species in NAS database\n",
    "nas_species_list = pd.read_excel(r'speciesListNAS.xlsx', dtype = str)\n",
    "nas_species_list = nas_species_list[['species.id', 'scientific.name', 'common.name', 'native.exotic']]\n",
    "\n",
    "# merge species list with NAS impacts to add species names\n",
    "merged_impact_data = pd.merge(impact_data, nas_species_list, left_on='Species ID', right_on='species.id', how='left')\n",
    "\n",
    "# convert text strings in 'Associated TSNs' column into a list\n",
    "merged_impact_data['Associated TSNs'] = merged_impact_data['Associated TSNs'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "\n",
    "# seperate each entry in 'Associated TSNs' column into its own row\n",
    "explode_data = merged_impact_data.explode('Associated TSNs')\n",
    "\n",
    "# the following may require R and taxize package to be installed \n",
    "# import taxize package from R\n",
    "taxize = importr(\"taxize\")\n",
    "\n",
    "# function to get scientific name from ITIS number\n",
    "def get_species(itis_number):\n",
    "    if pd.isna(itis_number):\n",
    "        return ''\n",
    "    else:     \n",
    "        try:\n",
    "            name = str(taxize.id2name(itis_number, db='itis')[0].rx2('name')).split('\"')[1]\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while retrieving species name for ITIS number {itis_number}: {e}\")\n",
    "            return 'Error'\n",
    "\n",
    "# apply custum get_species function to 'Associated TSNs'\n",
    "explode_data['Impacted Species'] = explode_data['Associated TSNs'].apply(get_species)\n",
    "\n",
    "# rename columns\n",
    "explode_data = explode_data.rename(columns={'scientific.name': 'Scientific Name', 'common.name': 'Common Name', 'native.exotic':'Status'})\n",
    "\n",
    "# reorder and select columns\n",
    "NAS_impact_data = explode_data[['Impact ID', 'Species ID', 'Scientific Name', 'Common Name', 'Status', 'Impact Type', 'Impact Description', 'Study Type', 'Study Location', 'Reference Number', 'Associated TSNs', 'Impacted Species', 'Great Lakes Region', 'Geographic Location', 'Notes']]\n",
    "\n",
    "# remove <em> hypertext - makes easier to read impact descriptions\n",
    "NAS_impact_data.loc[:,'Impact Description'] = NAS_impact_data['Impact Description'].str.replace('<em>', '').str.replace('</em>', '')\n",
    "\n",
    "# remove line breaks\n",
    "NAS_impact_data.loc[:, 'Impact Description'] = NAS_impact_data['Impact Description'].str.replace('\\n', '')\n",
    "\n",
    "# export to Excel\n",
    "NAS_impact_data.to_excel('NAS_output_' + str(date.today()) + '.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a9e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981a629d",
   "metadata": {},
   "source": [
    "### Clean GLANSIS Impact Data\n",
    "Subset the GLANSIS species from the NAS species. Drop and replace the status column because GLANSIS uses different terms for invasive species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937406ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\redinger\\AppData\\Local\\Temp\\1\\ipykernel_27412\\1641211976.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_impact_data.drop(columns = ['Status'], inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# read in GLANSIS species list\n",
    "glansis_species_list = pd.read_excel(r'speciesListGLANSIS.xlsx', dtype = str)\n",
    "\n",
    "# create a list of species id's to subset NAS dataset\n",
    "glansis_subset_list = list(glansis_species_list['species.id'])\n",
    "\n",
    "# subset NAS dataset to get only GLANSIS species impacts\n",
    "subset_impact_data = NAS_impact_data[NAS_impact_data['Species ID'].isin(glansis_subset_list)]\n",
    "\n",
    "# drop NAS Status column - GLANSIS uses slightly different status terminology\n",
    "subset_impact_data.drop(columns = ['Status'], inplace = True)\n",
    "\n",
    "# merge glansis_species_list with subset_impact_data to get GLANSIS status and group\n",
    "merged_impact_data = pd.merge(subset_impact_data, glansis_species_list, left_on='Species ID', right_on='species.id', how='left')\n",
    "\n",
    "# rename columns\n",
    "merged_impact_data = merged_impact_data.rename(columns={'status':'Status', 'group':'Group'})\n",
    "\n",
    "# reorder and select columns\n",
    "GLANSIS_impact_data = merged_impact_data[['Impact ID', 'Species ID', 'Scientific Name', 'Common Name', 'Group', 'Status', 'Impact Type', 'Impact Description', 'Study Type', 'Study Location', 'Reference Number', 'Associated TSNs', 'Impacted Species', 'Great Lakes Region', 'Geographic Location', 'Notes']]\n",
    "\n",
    "# export GLANSIS impact data to Excel\n",
    "GLANSIS_impact_data.to_excel('GLANSIS_output_' + str(date.today()) + '.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df5785",
   "metadata": {},
   "source": [
    "### *Coregonus Artedi* selection for panal review\n",
    "This pulls are impacts related to cisco. This can be modified for other species reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "056a5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask on TSN numbers\n",
    "itis_mask = GLANSIS_impact_data['Associated TSNs'].str.contains(r'\\b623384\\b', case = False)\n",
    "\n",
    "# create masks on Impact Descriptions\n",
    "science_name_mask = GLANSIS_impact_data['Impact Description'].str.contains(r'\\bCoregonus artedi\\b', case = False)\n",
    "abbrev_mask = GLANSIS_impact_data['Impact Description'].str.contains(r'\\bC. artedi\\b', case = False)\n",
    "cisco_mask = GLANSIS_impact_data['Impact Description'].str.contains(r'\\bcisco\\b', case = False)\n",
    "lakeherring_mask = GLANSIS_impact_data['Impact Description'].str.contains(r'\\blake herring\\b', case = False)\n",
    "\n",
    "# apply masks to GLANSIS data\n",
    "masked_data = GLANSIS_impact_data[cisco_mask | science_name_mask | abbrev_mask | lakeherring_mask | itis_mask ]\n",
    "\n",
    "# handle duplications - this removed duplicated from 'exploded' data\n",
    "\n",
    "# export cisco data to excel\n",
    "cisco_data.to_excel('cisco_impact_data_' + str(date.today()) + '.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70a5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0eb9234",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d914b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Impact_ID                Species\n",
      "0          1        Coregonus albus\n",
      "4          5            Salmo salar\n",
      "5          3    Coregonus lavaretus\n",
      "6          7  Coregonus oxyrhynchus\n"
     ]
    }
   ],
   "source": [
    "# Sample dataframe\n",
    "data = {\n",
    "    'Impact_ID': [1, 2, 3, 2, 5, 3, 7],\n",
    "    'Species': ['Coregonus albus', 'Salmo trutta', 'Salmo trutta', 'Salvelinus fontinalis', 'Salmo salar', 'Coregonus lavaretus', 'Coregonus oxyrhynchus']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to filter duplicates based on conditions\n",
    "def filter_duplicates(df):\n",
    "    # Identify duplicates\n",
    "    duplicates = df[df.duplicated(subset='Impact_ID', keep=False)]\n",
    "    \n",
    "    # Identify duplicates that are not Coregonus species\n",
    "    non_coregonus_duplicates = duplicates[~duplicates['Species'].str.contains('Coregonus')]\n",
    "    \n",
    "    # Get indexes of duplicates that need to be removed\n",
    "    indexes_to_remove = non_coregonus_duplicates.index\n",
    "    \n",
    "    # Filter out rows with indexes to remove\n",
    "    filtered_df = df.drop(indexes_to_remove)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Apply function\n",
    "filtered_df = filter_duplicates(df)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a423d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impact_ID</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Coregonus albus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Salmo trutta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Salmo trutta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Salvelinus fontinalis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Salmo salar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Coregonus lavaretus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Coregonus oxyrhynchus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Impact_ID                Species\n",
       "0          1        Coregonus albus\n",
       "1          2           Salmo trutta\n",
       "2          3           Salmo trutta\n",
       "3          2  Salvelinus fontinalis\n",
       "4          5            Salmo salar\n",
       "5          3    Coregonus lavaretus\n",
       "6          7  Coregonus oxyrhynchus"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f33f26ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Impact_ID                Species\n",
      "0          1        Coregonus albus\n",
      "3          2  Salvelinus fontinalis\n",
      "4          5            Salmo salar\n",
      "5          3    Coregonus lavaretus\n",
      "6          7  Coregonus oxyrhynchus\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Impact_ID': [1, 2, 3, 2, 5, 3, 7],\n",
    "    'Species': ['Coregonus albus', 'Salmo trutta', 'Salmo trutta', 'Salvelinus fontinalis', 'Salmo salar', 'Coregonus lavaretus', 'Coregonus oxyrhynchus']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a mask to identify rows with 'Coregonus'\n",
    "coregonus_mask = df['Species'].str.contains('Coregonus')\n",
    "\n",
    "# Create a mask to identify duplicate Impact_ID\n",
    "duplicate_mask = df.duplicated(subset='Impact_ID', keep=False)\n",
    "\n",
    "# Create a mask to identify the first duplicate row without 'Coregonus'\n",
    "first_duplicate_non_coregonus_mask = df[duplicate_mask & ~coregonus_mask].duplicated(subset='Impact_ID', keep='first')\n",
    "\n",
    "# Apply conditions to filter the dataframe\n",
    "filtered_df = df[~((duplicate_mask & ~coregonus_mask) & ~first_duplicate_non_coregonus_mask) | coregonus_mask]\n",
    "\n",
    "print(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvImpactPullNAS",
   "language": "python",
   "name": "venvimpactpullnas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
